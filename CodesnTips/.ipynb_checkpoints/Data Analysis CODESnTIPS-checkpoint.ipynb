{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "89b03c3b-fed5-4093-987f-648529c00869"
    }
   },
   "source": [
    "# DATA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "08904d6c-4a4c-4767-b76c-ef5eb4500d13"
    }
   },
   "source": [
    "Different type of **Exploratory Data Analysis(EDA)**:\n",
    " * Univariate nongraphical\n",
    " * Multivariate nongraphical\n",
    " * Univariate graphical\n",
    " * Multivariate graphical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ecc80eda-2b15-479b-ad57-eec5ea823222"
    }
   },
   "source": [
    "### Checking data by having a glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b17582de-0566-4ca2-9be9-d0abc9601daa"
    }
   },
   "outputs": [],
   "source": [
    "str(df)\n",
    "dim(df)\n",
    "describe(df)\n",
    "\n",
    "# Unique values per column\n",
    "lapply(df, function(x) length(unique(x)))\n",
    "\n",
    "# Fast way to check NAs in each column of dataframe\n",
    "colSums(is.na(df))\n",
    "# or (as alternative)\n",
    "sapply(df, function(x)sum(is.na(x)))\n",
    "       \n",
    "#Check the percentage of Missing values       \n",
    "missing_values <- df %>% summarize_all(funs(sum(is.na(.))/n()))\n",
    "missing_values <- gather(missing_values, key=\"feature\", value=\"missing_pct\")\n",
    "       \n",
    "## Another approach for getting a full vision of dataframe status\n",
    "## is using funModeling library\n",
    "library(funModeling)\n",
    "df_status(df)\n",
    "# Retrieves the distribution in a table and a plot and shows the distribution of absolute and relative numbers\n",
    "freq(data=df, input = c('colname1','colname2',...))\n",
    "# Full numerical profiling in one function automatically excludes non-numerical variables\n",
    "profiling_num(data_world_wide)\n",
    "# Plots the distribution of every numerical variable while automatically excluding the non-numerical ones       \n",
    "plot_num(data_world_wide)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cea36197-f19c-4f05-afa2-e90494a97040"
    }
   },
   "source": [
    "### Data Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4bd5cdb5-11ff-495f-9ca0-0cc823486014"
    }
   },
   "outputs": [],
   "source": [
    "## Select exclusively (select all columns except mentioned)\n",
    "select(df, -one_of(colname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "70cf1577-0ea7-495e-b9ea-2147b3356ab0"
    }
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a384db74-40fe-480b-ab16-9a2c67a8c2ef"
    }
   },
   "outputs": [],
   "source": [
    "# To count the occurrence of each value\n",
    "table(df$colname)\n",
    "\n",
    "# To get the percentage of occurence\n",
    "prop.table(table(df$colname))\n",
    "       \n",
    "# In order to get most frequent value in a column of df\n",
    "tail(names(sort(table(df$colname))), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e33eff06-8eff-428f-af51-0b987be92eb5"
    }
   },
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dbcbfdef-933e-4d59-a2ad-198d7e1c7832"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion table for two categorical variable in dataset\n",
    "with(df, CrossTable(colname1, colname2))\n",
    "\n",
    "# In order to show relation between two contiuous variable we can do the scatterplot\n",
    "ggplot(data = df, aes(x = colname1 , y = colname2)) + geom_point()\n",
    "\n",
    "# For a combination of continuous and categorical variables we can use boxplot\n",
    "ggplot(data = df, aes(x = colname1 , y = colname2)) + geom_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "773f4421-6b58-40b3-8dd7-212c18dac079"
    }
   },
   "source": [
    "### Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d3490f30-88b1-45eb-a4f5-5e0ad593d9e8"
    }
   },
   "outputs": [],
   "source": [
    "# One approach for spoting outliners are the using geom_jitter\n",
    "ggplot(df, aes(colname1, colname2)) + \n",
    "    geom_jitter(alpha = 0.2 ,size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "08bfd091-3759-4f3c-a8de-a6c1e2dc00b1"
    }
   },
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6f2f23b4-c09c-486f-a3ec-951beeccb6be"
    }
   },
   "source": [
    "#### Data reduction\n",
    "If you use a large amount of data, it has bigger data size and needs more resources, moreover it may produce redundant results. In order to overcome such difficulties, we can use data reduction methods. hence, the storage efficiency will increase and at the same time we can minimize the data handling costs and will minimize the analysis time also. There are several types of data reduction:\n",
    " - **Filtering and sampling**\n",
    "     - Moving average filtering\n",
    "     - Savitzky-Golay filtering\n",
    "     - High correlation filtering\n",
    "     - Bayesian filtering\n",
    " - **Binned algorithm**\n",
    "     - Equal-width binning\n",
    "     - Equal-size binning\n",
    "     - Optima binning\n",
    "     - Multi-interval discretization binning\n",
    " - **Dimensionality reduction**\n",
    "     - Principle Component Analysis (PCA)\n",
    "     - Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d9ff1cf4-4779-4e07-a555-9dca8a762b24"
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7dcf7519-459c-4a04-bd4d-fc6d3fb7e8da"
    }
   },
   "source": [
    "Useful Data minging/Cleaning tools for preparing messy data for analysis in R or python:\n",
    " - [OpenRefine](https://github.com/OpenRefine/OpenRefine)\n",
    " - [DataWrangler](http://vis.stanford.edu/wrangler/)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "70281bb3-3b4f-4015-a2a3-8d6e54fa1e8b"
    }
   },
   "outputs": [],
   "source": [
    "# Renaming field names by using dplyr\n",
    "df <- df %>% rename(\n",
    "  newname1 = oldname1,\n",
    "  newname2 = oldname2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c35d061c-89f4-4ab7-88c6-d52107c5c2e1"
    }
   },
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a80fb08-d6dc-4a3e-b981-8f165b0d6573"
    }
   },
   "source": [
    "#### Removing NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "991bc5a7-fb25-422d-801e-d87f92f02dfb"
    }
   },
   "outputs": [],
   "source": [
    "df <- na.omit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f0158fc4-db9b-43b6-9a2e-b994e5635df9"
    }
   },
   "source": [
    "#### Imputing NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "883f2efe-259f-421b-addf-9cab3c2d70d3"
    }
   },
   "outputs": [],
   "source": [
    "## First approach: simply replace NAs with mean of the column\n",
    "df <- df %>%\n",
    "    mutate( colname = ifelse(is.na(colname), \n",
    "                             mean(df$colname, na.rm=TRUE), \n",
    "                             colname))\n",
    "\n",
    "# Use the most common value to replace NAs in the desired feature. \n",
    "# Imagine S was the most frequent \n",
    "# value for that field within the dataframe \n",
    "df$colname <- replace(df$colname, \n",
    "                      which(is.na(df$colname)), \n",
    "                      'S')\n",
    "\n",
    "# One sublte way to filling null values in data set would be using\n",
    "# prediction based on other explanatory variables. For example, \n",
    "# here we're looking for a countinous variable based on others.\n",
    "exp_var_filler_model <- rpart(exp_var_with_many_NA ~ a_list_of_exp_vars,\n",
    "                              data=df[!is.na(df$exp_var),], \n",
    "                              method=\"anova\" #since we consider that exp_var as continous, otherwise we could use `class`)\n",
    "df$exp_var[is.na(df$exp_var)] <- predict(exp_var_filler_model, \n",
    "                                         [is.na(df$exp_var),])\n",
    "                              \n",
    "## Third approach: using mice\n",
    "library(mice)\n",
    "simple_df_with_NA = df[c(\"NAcol1\", \"NAcol2\", \"col1\", \"col2\")]\n",
    "imputed = complete(mice(simple_df_with_NA))\n",
    "df$NAcol1 = imputed$NAcol1\n",
    "df$NAcol2 = imputed$NAcol2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a4f1c16b-9def-4351-a239-73305f205db3"
    }
   },
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "df7557ee-3214-4eb9-be25-cc378cc4e90b"
    }
   },
   "outputs": [],
   "source": [
    "# Having a random sample dataframe from the original data by having only 3 row[for sake of example]\n",
    "df_sample <- df[sample(nrow(df), 3), ]\n",
    "\n",
    "# Using dplyr to have the same result as above\n",
    "df_sample <- sample_n(df, size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "568223fa-19ff-4e07-b536-8361dd9bf867"
    }
   },
   "source": [
    "# Data Manipulation (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "811785dc-ae7f-45cf-81a9-f277e0f59ef8"
    }
   },
   "source": [
    "### Feature Engineering\n",
    "Feature engineering has been described as easily the most important factor in determining the success or failure of your predictive model. Feature engineering really boils down to the human element in machine learning. How much you understand the data, with your human intuition and creativity, can make the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e9b89cbb-eb09-4c31-8ac6-ae2d2b6cc1cd"
    }
   },
   "outputs": [],
   "source": [
    "df <- df %>%\n",
    "    mutate( new_feature = case_when(raw_feature < 13 ~ \"Lower.Range\", \n",
    "                                    raw_feature >= 13 & raw_feature < 18 ~ \"Mid.Range.1\",\n",
    "                                    raw_feature >= 18 & raw_feature < 60 ~ \"Mid.Range.2\",\n",
    "                                    raw_feature >= 60 ~ \"Upper.Range\"))\n",
    "\n",
    "# In order to combine same categorical data which has very little proportion from total we can use recode method\n",
    "libray(car)\n",
    "df$cat_var <- recode(df$cat_var, \"c('group1', 'group2',...) = 'more_general_group'\")\n",
    "# Or simply\n",
    "df3$cat_var[df$cat_var %in% c('group1','group2')] = 'more_general_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dedd4808-be36-485c-b194-786284c5c6fa"
    }
   },
   "source": [
    "# Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ec211856-d3bf-44ea-9155-058a81bab5bf"
    }
   },
   "outputs": [],
   "source": [
    "# find the number of survivors for the different subsets\n",
    "> aggregate(Survived ~ Child + Sex, data=train, FUN=sum)\n",
    "  Child    Sex Survived\n",
    "1     0 female      195\n",
    "2     1 female       38\n",
    "3     0   male       86\n",
    "4     1   male       23\n",
    "\n",
    "# To know the total number of people in each subset\n",
    "> aggregate(Survived ~ Child + Sex, data=train, FUN=length)\n",
    "  Child    Sex Survived\n",
    "1     0 female      259\n",
    "2     1 female       55\n",
    "3     0   male      519\n",
    "4     1   male       58\n",
    "\n",
    "# To know the proportions\n",
    "> aggregate(Survived ~ Child + Sex, data=train, \n",
    "            FUN=function(x) {sum(x)/length(x)})\n",
    "  Child    Sex  Survived\n",
    "1     0 female 0.7528958\n",
    "2     1 female 0.6909091\n",
    "3     0   male 0.1657033\n",
    "4     1   male 0.3965517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "65d33720-a356-4613-9ebc-0c63d1ff8c8b"
    }
   },
   "source": [
    "# Correlation and Relationship\n",
    "Perhaps the most standard correlation measure for numeric variables is the R statistic (or Pearson coefficient) which goes from 1 positive correlation to -1 negative correlation. A value around 0 implies no correlation.\n",
    "\n",
    "Squaring this number returns the **R-squared** statistic (aka **R2**), which goes from 0 no correlation to 1 high correlation.\n",
    "\n",
    "> R statistic is highly influenced by outliers and non-linear relationships. Highly recommended to **Plot** data as checking correlation as well as thinking about outliners before evaluating correlation.**\n",
    "\n",
    "\n",
    "**<font color=blue>NOTE:</font>** Correlation can be measure better with Information Theory concepts. One of the many algorithms to measure correlation based on this is: MINE, acronym for: Maximal Information-based nonparametric exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "80769103-66b7-4304-b9e0-f9e81e9f3696"
    }
   },
   "outputs": [],
   "source": [
    "## For evaluating correlation between two columns in dataset\n",
    "cor(df$colname1, df$colname2)\n",
    "\n",
    "## For getting the correlation table between all the columns of dataset\n",
    "cor(df)\n",
    "\n",
    "## By using funModeling package\n",
    "library(funModeling)\n",
    "correlation_table(data=df, target=\"colname\")\n",
    "\n",
    "## Using MINE algorithm\n",
    "library(minerva)\n",
    "res_mine <- df %>%\n",
    "            select_if(is.numeric) %>%\n",
    "            mine(use = 'pairwise.complete.obs')\n",
    "res_mine$MIC\n",
    "\n",
    "## MINE can also help us to profile time series regarding its non-monotonicity with MAS (maximum asymmetry score).\n",
    "# MAS ~ 0 indicates monotonic function\n",
    "# MAS ~ 1 indicates non-monotonic function (always up or down)\n",
    "res_mine$MAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "802fcf35-a0ff-4924-80ef-77d2c26a6d57"
    }
   },
   "source": [
    "###  Correlation on categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "04acf643-85dc-4335-b4a1-9f83a3be7b4e"
    }
   },
   "source": [
    "MINE -and many other algorithms- only work with numerical data. We need to do a data preparation trick, converting every categorical variable into flag (or dummy variable).\n",
    "\n",
    "If the original categorical variable has 30 possible values, it will result in 30 new columns holding the value 0 or 1, when 1 represents the presence of that category in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "85882a6f-25a0-497d-9f7f-764b2f557986"
    }
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "\n",
    "# selecting just categorical column(s) which we're going to convert\n",
    "df_cat=select(df, cat_colname1, cat_colname2, ...)\n",
    "# it converts all categorical variables (factor and character for R) into numerical variables\n",
    "# skipping the original so the data is ready to use\n",
    "dmy = dummyVars(\" ~ .\", data = df_cat)\n",
    "df_cat_explode = data.frame(predict(dmy, newdata = df_cat))\n",
    "# Important: If you recieve this message `Error: Missing values present in input variable 'x'. \n",
    "# Consider using use = 'pairwise.complete.obs'.` is because data has missing values.\n",
    "# Please don't omit NA without an impact analysis first, in this case it is not important. \n",
    "df_cat_explode = na.omit(df_cat_explode)\n",
    "# compute the mic!\n",
    "mine_res = mine(df_cat_explode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3e2e5e3a-e3b0-4a68-a461-7dffa809348b"
    }
   },
   "source": [
    "### Visualizing the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f3350e6c-f6c1-4b4e-9ddd-97212f600f5f"
    }
   },
   "outputs": [],
   "source": [
    "## The corrplot package is a graphical display of a correlation matrix, confidence interval.\n",
    "library(corrploy)\n",
    "\n",
    "## simple case 1: having simple cor function as data provider\n",
    "df %>%\n",
    "  select_if(is.numeric) %>%\n",
    "  cor(use=\"complete.obs\") %>%\n",
    "  corrplot.mixed(tl.cex=0.85)\n",
    "\n",
    "# simple case 2: plotting MIC results by using circle method\n",
    "corrplot(mine_res$MIC, method=\"circle\",\n",
    "         col=brewer.pal(n=10, name=\"PuOr\"),\n",
    "         # only display upper diagonal\n",
    "         type=\"lower\", \n",
    "         #label color, size and rotation\n",
    "         tl.col=\"red\", tl.cex = 0.9, tl.srt=90, \n",
    "         # dont print diagonal (var against itself)\n",
    "         diag=FALSE, \n",
    "         # accept a any matrix, mic in this case (not a correlation\n",
    "         #   element)\n",
    "         is.corr = F)\n",
    "\n",
    "## simple case 3: same as last case but by showing correlation values on plot\n",
    "corrplot(mine_res$MIC, method=\"color\",\n",
    "         type=\"lower\", number.cex=0.7,\n",
    "         addCoef.col = \"black\", # Add coefficient of correlation\n",
    "         tl.col=\"red\", tl.srt=90, tl.cex = 0.9,\n",
    "         diag=FALSE, is.corr = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f68c8769-5f4e-459c-b080-8504d24e7279"
    }
   },
   "source": [
    "# Regression\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ae0e2c4a-4b27-48c6-92bb-cf85baae9e28"
    }
   },
   "outputs": [],
   "source": [
    "fit <- lm(outcome ~ independent1 + independent2 + ..., data = train_df)\n",
    "## Getting information about the model\n",
    "summary(fit)\n",
    "\n",
    "## Calculating Model R-squared\n",
    "Model_R2 <- sum(fit$residuals^2)\n",
    "\n",
    "## Calculating R-squared of out-of-same prediction\n",
    "pred <- predict(fit, newdata=test_df)\n",
    "SSE = sum((pred - test_df$outcome)^2)\n",
    "# Consider the baseline is based on train dataset\n",
    "SST = sum((test_df$outcome - mean(train_df$outcome))^2)\n",
    "R2 = 1 - SSE/SST\n",
    "RMSE = sqrt(SSE/nrow(test_df))\n",
    "RMSE = sqrt(mean((pred - test_df$outcome)^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d2169b55-e9c5-448f-bba6-48678e50a139"
    }
   },
   "source": [
    "#### Simplicity the model\n",
    "That is, a model with fewer variables is preferable to a model with many unnnecessary variables. \n",
    "Experiment with removing independent variables from the original model. We can use the significance of the coefficients to decide which variables to remove (remove the one with the <font color=red>**largest p-value**</font> first, or the one with the <font color=red>**t-value closest to zero**</font>), and to remove them one at a time (this is called \"backwards variable selection\"). This is important due to **multicollinearity** issues. <br/>\n",
    "Removing one insignificant variable may make another previously insignificant variable become significant.\n",
    "\n",
    "**NOTE:** When we remove insignificant variables, the \"Multiple R-squared\" will always be worse, but only slightly worse. This is due to the nature of a linear regression model. It is always possible for the regression model to make a coefficient zero, which would be the same as removing the variable from the model. The fact that the coefficient is not zero in the intial model means it must be helping the R-squared value, even if it is only a very small improvement. So when we force the variable to be removed, it will decrease the R-squared a little bit. However, this small decrease is worth it to have a simpler model.\n",
    "On the contrary, when we remove insignificant variables, the \"Adjusted R-squred\" will frequently be better. This value accounts for the complexity of the model, and thus tends to increase as insignificant variables are removed, and decrease as insignificant variables are added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a31f7f07-3385-4cd3-af77-d9bb70131b0d"
    }
   },
   "outputs": [],
   "source": [
    "## If we don't have any idea about which independent variables \n",
    "# should be involved in formula, we can involve all and then using\n",
    "# step()  method to simplify the complex model to simpler one.\n",
    "messy_model <- lm(outcome ~ ., data = df)\n",
    "simplified_model <- step(messy_model)\n",
    "summary(simplified_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4083a262-15be-4362-b6ab-99b5cd1766cf"
    }
   },
   "source": [
    "#### Unordered factors in regression models\n",
    "To include unordered factors in a linear regression model, we define one level as the \"reference level\" and add a binary variable for each of the remaining levels. In this way, a factor with **n** levels is replaced by **n-1** binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.\n",
    "\n",
    "As an example, consider the unordered factor variable `color`, with levels `red`, `green`, and `blue`. If \"green\" were the reference level, then we would add binary variables \"colorred\" and \"colorblue\" to a linear regression problem. All red examples would have `colorred=1` and `colorblue=0`. All blue examples would have `colorred=0` and `colorblue=1`. All green examples would have `colorred=0` and `colorblue=0`.\n",
    "\n",
    "**Hint:** by default R selects the first level alphabetically as the reference level of our factor instead of the most common level. We can set the reference level of the factor by typing the following two lines in your R console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "49e12bcb-1efc-44c5-90ff-4380eca26287"
    }
   },
   "outputs": [],
   "source": [
    "## Setting reference level in unordered factor variable in dataset\n",
    "df$factor_colname = relevel(df$factor_colname, \"MostFreqColName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "82719178-2992-487e-91cf-34ceeee937e0"
    }
   },
   "source": [
    "#### Modeling on skewed dependent variable\n",
    "\n",
    "When handling a **skewed** dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself -- this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models, which can be computed in R using the log() function.\n",
    "\n",
    "> fit <- lm(log(outcome) ~ colname, data=df)\n",
    "\n",
    "However, the dependent variable in our model is log(outcome), so `Prediction` would contain predictions of the log(outcome) value. We are instead interested in obtaining predictions of the outcome value. We can convert from predictions of log(outcome) to predictions of outcome via exponentiation, or the `exp()` function. \n",
    "\n",
    "> Prediction = exp(predict(fit, newdata=test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6f7ace70-dbd8-4ac8-91d4-9a6c2cd4020b"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "Used for nonlinear dependent varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ea14691b-ace4-4fc5-9c6c-4478fca42ac0"
    }
   },
   "outputs": [],
   "source": [
    "## Spliting data into Train and test set\n",
    "library(caTools)\n",
    "# First arg: the columns which we want to split on\n",
    "# Second arg: the ratio of rows in train and test sets\n",
    "splitIndex <- sample.split(df$outcome, SplitRatio=0.75)\n",
    "df_Train <- subset(df, splitIndex == TRUE)\n",
    "df_Test  <- subset(df, splitIndex == FALSE)\n",
    "\n",
    "## Making the logistic regression model by using glm\n",
    "logit <- glm(outcome ~ colname1 + colname2 + ..., \n",
    "             data=df_Train,\n",
    "             family=binomial)\n",
    "## We need to look for model with minimum AIC on the same training dataset\n",
    "summary(logit)\n",
    "\n",
    "## Making prediction based on Training set\n",
    "predictTrain <- predict(logit, type=\"response\")\n",
    "\n",
    "## For making Confusion matrix \n",
    "table(df_Train$outcome, predictTrain > threshold)\n",
    "\n",
    "## In order to choose best value for threshold we can plot the ROC curve\n",
    "library(ROCR)\n",
    "ROCRpred = prediction(predictTrain, df$outcome)\n",
    "ROCRpref = performance(ROCRpred, \"tpr\", \"fpr\")\n",
    "plot(ROCRpref, \n",
    "     colorize = TRUE, \n",
    "     print.cutoffs.at=seq(0,1,0.1), \n",
    "     text.adj=c(-0.2,1.7))\n",
    "\n",
    "## For evaluating the quality of model we can check the value of AUC\n",
    "# having AUC closer to 1.0 means we have better model\n",
    "AUC = as.numeric(performance(ROCRpred, \"auc\")@y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c1459ed4-2e81-4550-8d68-38a0304684e5"
    }
   },
   "source": [
    "#### TIP: Selecting a range of independent variables for modeling\n",
    "Sometime we need to select only a group of independent variables amoung a large number of variables in dataset which doing it with hand could be a tidious job. In order to achive that, we need to make a subset of variables which we want to use in model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4e26a6fd-4a92-4938-b1cb-9eb6daeb7bed"
    }
   },
   "outputs": [],
   "source": [
    "## First approach:\n",
    "excluded_vars = c(\"colname1\", \"colname2\", ...)\n",
    "min_df <- df[ , !(names(df) %in% excluded_vars) ]\n",
    "model <- glm(outcome~., data=min_df)\n",
    "\n",
    "## Second approach (Just work on removing numeric variables):\n",
    "model <- glm(outcome~.-colname1-colname2, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bfbe692-afe1-4aa1-a811-4536975af9f2"
    }
   },
   "source": [
    "# Learning and classification\n",
    "Generally there are two kind of classifier:\n",
    " - **Binary classification**\n",
    " - **Multiclass classification**\n",
    "\n",
    "Most frequent algorithm for classification:\n",
    " - Support vector machines\n",
    " - Neural networks\n",
    " - Decision trees\n",
    " - NaÃve Bayes\n",
    " - Hidden Markov models\n",
    " \n",
    "#### NaÃve Bayes\n",
    "In this algorithm, we simply need to learn the probabilities by making the assumption that the attributes A and B **are independents**, hence the reason this model is defined as an independent feature model. NaÃve Bayes is widely used in text classification because the algorithm can be trained easily and efficiently. In NaÃve Bayes, we can calculate the probability of a condition A if B $(P(A|B))$, if you already know the probability of B given A $(P(B|A))$, as well as to A $(P(A))$ and B $(P(B))$ individually, as is shown in the previous Bayes Theorem example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0c7f3976-bce5-486f-a683-31087c851f70"
    }
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f5d2105c-956f-49bd-84a3-ef635818114a"
    }
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "# Using rpart for list of explanatory variables for a discreate outcome\n",
    "# class method (for ones and zeros output)\n",
    "decisionTree <- rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "                      data=df_train,\n",
    "                      method=\"class\"#means use classification tree instead of regression tree,\n",
    "                      minbucket=25 #determines the minimum number of observations in each bucket)\n",
    "# To visualise the tree in screen\n",
    "prp(decisionTree)\n",
    "\n",
    "# If you wanted to predict a continuous variable as outcome\n",
    "rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "               data=df_train,\n",
    "               method=\"anova\")\n",
    "\n",
    "## Making prediction on test dataset\n",
    "Prediction <- predict(decisionTree, \n",
    "                      newdata=df_test, \n",
    "                      type=\"class\" #means giving the threshold of 0.5)\n",
    "table(df_test$outcome, Prediction)\n",
    "\n",
    "## To evaluate the model\n",
    "library(ROCR)\n",
    "PredictROC <- predict(decisionTree, newdata=df_test)\n",
    "pred <- prediction(PredictROC[,2], df_test$outcome)\n",
    "perf <- performance(pred, \"tpr\",\"fpr\")\n",
    "plot(perf)\n",
    "# To get the AUC of the model\n",
    "as.numeric(performance(pred, \"auc\")@y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a0cbfdd-3d2b-4dd4-898d-10f0f889c105"
    }
   },
   "source": [
    "# Overfitting\n",
    "Overfitting is technically defined as a model that performs better on a training set than another simpler model, but does worse on unseen data.\n",
    "\n",
    "In general, we can set the `minbucket` value in rpart to define what should be the minimum number of observation in each bucket but the problem is the don't know this value. We can use the `k-fold croos-validation` technique to figure out that value.\n",
    "\n",
    "In order to define this value in constructing model instead of minbucket we can set the `cp` value which mean **complexity parameter** which we can adjust the trade-off between model complexity and accuracy.\n",
    "\n",
    "**NOTE:** Smaller cp leads to a bigger tree (might overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a830ba4b-f28e-4712-8963-cda1ac762845"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to load libraries\n",
    "library(caret)\n",
    "library(e1071)\n",
    "\n",
    "# Then we need to define how many folds we want to have for cross-validation\n",
    "numFolds <- trainControl(method=\"cv\",\n",
    "                         number=10)\n",
    "# Then we need to define possible cp values by using expand.grid\n",
    "cpGrid <- expand.grid(.cp=seq(0.01,0.5,0.01)) # or any other ranges\n",
    "# Now, we are ready to perform cross validation\n",
    "train(outcome ~ exp_var1 + exp_var2 + ..., \n",
    "      data=df_train,\n",
    "      method=\"rpart\",\n",
    "      trControl=numFolds,\n",
    "      tuneGrid=cpGrid)\n",
    "# Now, we can construct our model by using cp instead of minbucket\n",
    "rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "               data=df_train,\n",
    "               method=\"class\", \n",
    "               cp='TheOutcomeOfAboveCommand') #e.g. cp=0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a406a493-0c10-4ec2-9cf4-0bf27c0b7a01"
    }
   },
   "source": [
    "# Prediction\n",
    "\n",
    "Based on problem:\n",
    "- If it's a classification problem, then we can use:\n",
    "    * Logistic Regression\n",
    "    * Naive Bayes\n",
    "    * Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "774ca79f-ca42-4e7b-8662-9ff80d1b90eb"
    }
   },
   "source": [
    "## Ensemble\n",
    "Take a large collection of individually imperfect models, and their one-off mistakes are probably not going to be made by the rest of them. If we average the results of all these models, we can sometimes find a superior model from their combination than any of the individual parts. That’s how ensemble models work, they grow a lot of different models, and let their outcomes be averaged or voted across the group.\n",
    "\n",
    "### Random Forest\n",
    "Random Forest models grow trees much deeper than the decision stumps above, in fact the default behaviour is to grow each tree out as far as possible. But since the formulas for building a single decision tree are the same every time, some source of randomness is required to make these trees different from one another. Random Forests do this in two ways.\n",
    " 1. The first trick is to use bagging, for bootstrap aggregating. Bagging takes a randomized sample of the rows in your training set, with replacement.\n",
    " 2. The second source of randomness gets past this limitation though. Instead of looking at the entire pool of available variables, Random Forests take only a subset of them, typically the square root of the number available. In our case we have 10 variables, so using a subset of three variables would be reasonable. The selection of available variables is changed for each and every node in the decision trees. \n",
    " \n",
    "R’s Random Forest algorithm has a few restrictions that we did not have with our decision trees. The big one has been the elephant in the room until now, we have to clean up the missing values in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e9820dc3-d1c1-4968-ba51-a6a6cf65d39c"
    }
   },
   "outputs": [],
   "source": [
    "## Making an random forest model after data prepration process\n",
    "# we used as.factor for outcome variable in order to make sure\n",
    "# the output is same as classification\n",
    "fit <- randomForest(as.factor(outcome) ~ exp_var1 + exp_var2 + ...,\n",
    "                    data=df_train, \n",
    "                    importance=TRUE,\n",
    "                    nodsize=25, #same as minbucket in CART\n",
    "                    ntree=2000 #number of trees)\n",
    "\n",
    "# To look at what variables were important:\n",
    " varImpPlot(fit)\n",
    "             \n",
    "## To make the prediction \n",
    "Prediction <- predict(fit, newdata=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e7822fbd-3216-40f9-84c3-df68210d5bb4"
    }
   },
   "source": [
    "There’s two types of importance measures shown above. The accuracy one tests to see how worse the model performs without each variable, so a high decrease in accuracy would be expected for very predictive variables. The Gini one digs into the mathematics behind decision trees, but essentially measures how pure the nodes are at the end of the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1c2a288f-2586-4b34-8445-3fc79ba263df"
    }
   },
   "source": [
    "But let’s not give up yet. There’s more than one ensemble model. Let’s try a forest of conditional inference trees. They make their decisions in slightly different ways, using a statistical test rather than a purity measure, but the basic construction of each tree is fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "aecb0912-fa06-46c3-bff7-26c390d7b9aa"
    }
   },
   "outputs": [],
   "source": [
    "install.packages('party')\n",
    "library(party)\n",
    "\n",
    "fit <- cforest(as.factor(outcome) ~ exp_var1 + exp_var2 + ...,\n",
    "                 data = df_train, \n",
    "                 controls=cforest_unbiased(ntree=2000, mtry=3))\n",
    "\n",
    "Prediction <- predict(fit, test, OOB=TRUE, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c71ab1cb-e392-43b5-9dd2-c78ca7f35373"
    }
   },
   "source": [
    "\n",
    "# Text Analysis\n",
    "The first thing for test mining is cleaning the text by doing some pre-processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "aff43b6c-5389-476e-9431-35691b45674b"
    }
   },
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "library(tm)\n",
    "library(SnowballC)\n",
    "\n",
    "## First we need to construct corpus based on our data\n",
    "corpus <- Corpus((VectorSource(df$text_colname)))\n",
    "\n",
    "## Step1. Making the whole texts lowercase\n",
    "corpus <- tm_map(corpus, tolower)\n",
    "\n",
    "## Step2. Removing the puntuations\n",
    "corpus <- tm_map(corpus, removePunctuation)\n",
    "\n",
    "## Step3. Removing unhelpful words\n",
    "corpus <- tm_map(corpus, \n",
    "                 removeWords, \n",
    "                 c(\"LIST_OF_ANY_WORD_YOU_WANT_TO_REMOVE\", \n",
    "                   stopwords(\"english\"))) # stopwords(english) is an internal dictionanry of unhelpful words\n",
    "\n",
    "## Step4. Stem the document (it causes to mine the word roots)\n",
    "corpus <- tm_map(corpus, stemDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e1951182-1b3a-40f0-a102-939c3b18ddce"
    }
   },
   "source": [
    "After doing the pre-processing steps on the text, now we are ready to count the words frequency which will be used in prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7106889d-054a-45af-804f-d5965ef771d0"
    }
   },
   "outputs": [],
   "source": [
    "# From tm package we need to call below method to make a matrix of \n",
    "# documents as rows and words in each doc as columns\n",
    "frequencies <- DocumentTermMatrix(corpus)\n",
    "\n",
    "# In order to just take a look into a few rows and columns of frequencies we can do:\n",
    "inspect(frequencies[1000:1005, 505:515])\n",
    "\n",
    "# For having a list of most popular terms we can use:\n",
    "findFreqTerms(frequencies, lowfreq=20) #lowfreq determines the minimum number of frequency\n",
    "\n",
    "## Having to much terms as independent variable for making the prediction\n",
    "# model couldn't be useful, so, we need to shorten the terms by removing non-frequent ones\n",
    "sparse <- removeSparseTerms(frequencies, .995) #which means keep terms which are appear in more than 0.5 percent of all documents\n",
    "\n",
    "# Now, we need to convert sparse matrix to data-frame which could be used for predicting model\n",
    "docsSparse <- as.data.frame(as.matrix(sparse))\n",
    "# And in order to make sure all columns names started with character:\n",
    "colnames(docsSparse) = make.names(colnames(docsSparse))\n",
    "# Then adding the outcome variable to newly generated dataframe\n",
    "docsSparse$outcome <- df$outcome\n",
    "\n",
    "## spliting the data for making the train/test set\n",
    "split = sample.split(docsSparse$Negative, SplitRatio=.7)\n",
    "trainSparse = subset(docsSparse, split==TRUE)\n",
    "testSparse  = subset(docsSparse, split==FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "52151681-b635-45a4-b6f7-9eafbe93fdf2"
    }
   },
   "source": [
    "Now, we prepared our data, we can use CART to predict our model. Which is not something new rather than normal prediction by using rpart and randomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4fbd65ac-4e85-475b-bb22-4b63f8f0c6fd"
    }
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "docCART = rpart(outcome ~ ., data=trainSparse, method=\"class\")\n",
    "# To take a look at tree\n",
    "prp(tweetCART)\n",
    "# making the prediction \n",
    "predictCART <- predict(docCART, newdata=testSparse,type=\"class\")\n",
    "# Checking the accuracy\n",
    "table(testSparse$outcome, predictCART)\n",
    "       predictCART7\n",
    "        FALSE TRUE\n",
    "  FALSE   294    6\n",
    "  TRUE     37   18\n",
    "# Accuracy: (294+18)/(294+6+37+18) ~= .87\n",
    "# Compare to baseline\n",
    "table(testSparse$outcome)\n",
    "FALSE  TRUE \n",
    "  300    55\n",
    "# Accuracy: (300)/(300+55) ~= .84 means the model does better than baseline\n",
    "\n",
    "## making prediction by RandomForest\n",
    "library(randomForest)\n",
    "docRF <- randomForest(Negative ~ .,data=trainSparse)\n",
    "predictRF = predict(docRF, newdata=testSparse)\n",
    "# Checking the accuracy\n",
    "table(testSparse$Negative, predictRF)\n",
    "       predictRF\n",
    "        FALSE TRUE\n",
    "  FALSE   293    7\n",
    "  TRUE     34   21\n",
    "# Accuracy: (293+21)/(293+7+34+21) ~= .88 A bit better than CART but less interpretable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
