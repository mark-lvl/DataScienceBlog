{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sense out of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking dataframe columns, NaNs, memory usage\n",
    "print(df.info())\n",
    "\n",
    "## Getting summarize from numerical columns in dataset\n",
    "print(df.describe())\n",
    "\n",
    "## Checking for NAs\n",
    "df.isnull().sum()\n",
    "# Getting list of all NAs for a specific column\n",
    "df['COLUMN_WITH_NA'][np.isnan(df['COLUMN_WITH_NA'])]\n",
    "\n",
    "## Check value counts for categorical columns\n",
    "# Single column\n",
    "pd['COLUMN_NAME'].value_counts()\n",
    "# Multiple columns\n",
    "pd.crosstab(df['CAT_COL_1'], df['CAT_COL_2'])\n",
    "\n",
    "## With assert statement we can check a lot of thing. For example\n",
    "# assert data.columns[1] == 'Name'\n",
    "# assert data.Speed.dtypes == np.int\n",
    "assert df['COLUMN'].notnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting rows and columns in pandas\n",
    "df['COLUMN']['ROW']\n",
    "# similar to:\n",
    "df.COLUMN['ROW']\n",
    "# similar to:\n",
    "df.loc['ROW',['COLUMN']]\n",
    "# dataframe slicing\n",
    "df.loc[0:9,\"COL_1\":\"COL_n\"] #first 10 rows including col_1 to col_n\n",
    "df.loc[10:0:-1,\"COL_1\":\"COL_n\"] #first 10 rows/backward including col_1 to col_n\n",
    "\n",
    "## Selecting dataframe by having multiple conditions\n",
    "df[(df.COL1 > a) & (df.COL1 < b)]\n",
    "\n",
    "## Grouping dataframe rows based on some columns\n",
    "df[['COL_1', 'COL_2']].groupby(['COL_1'], as_index=False).mean()\n",
    "df.groupby('COL_1').COL_2.agg(['mean','min','max'])\n",
    "\n",
    "## Getting all columns except some\n",
    "# First approach\n",
    "df.loc[:, df.columns != 'EXCLUDE_COL']\n",
    "# Second approach\n",
    "df[df.columns.difference(['list_of_excluded_cols'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MISSING VALUES, NAs\n",
    "df[\"COLUMN\"].dropna(inplace = True)  # inplace = True means we do not assign it to new variable. Changes automatically assigned to data\n",
    "\n",
    "## Add new column to dataset on desired conditions\n",
    "# look in df and for rows with 'column_name' = 'some_value', set a 'new_column'  with 'desired_value'\n",
    "df.loc[df['COLUMN_NAME'] == 'SOME_VALUE', 'NEW_COLUMN'] = 'DESIRED_VALUE'\n",
    "\n",
    "## Convert numeric column into categories of range values\n",
    "# for having same number of records in each bin we use qcut\n",
    "df['NEW_CAT_COL'] = pd.qcut(df['NUM_COL'], 'FOLDS_NUM')\n",
    "# for having evenly spaced bins we use cut\n",
    "df['NEW_CAT_COL'] = pd.cut(df['NUM_COL'], 'FOLDS_NUM')\n",
    "\n",
    "## Filling NAs in dataframe\n",
    "df['COLUMN_NAME'] = df['COLUMN_NAME'].fillna('most_occurred_value')\n",
    "df['COLUMN_NAME'] = df['COLUMN_NAME'].fillna(df['COLUMN_NAME'].median())\n",
    "\n",
    "## Apply a method over a column\n",
    "# Titanic dataset: looking for title from Name column\n",
    "import re\n",
    "def title_finder(value):\n",
    "    title = re.search('([A-Za-z]+)\\.', value)\n",
    "    if title:\n",
    "        return title.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "train['Title'] = train['Name'].apply(title_finder)\n",
    "\n",
    "## Replacing values in column\n",
    "df['COLUMN'] = df['COLUMN'].replace(\"OLD_VALUE\", \"NEW_VALUE\") #old_value can come in list as well.\n",
    "\n",
    "## Searching in column values and replace\n",
    "df[\"COLUMN_NEW\"] = np.where(df[\"COLUMN\"].str.contains(\"SOME_STRING\"), 1, other=0)\n",
    "\n",
    "## Dropping a or a group of fields from dataframe\n",
    "drop_elements = ['COLUMN_1', 'COLUMN_2', 'COLUMN_3']\n",
    "df = df.drop(drop_elements, axis = 1)\n",
    "\n",
    "## Melt\n",
    "# id_vars = what we do not wish to melt\n",
    "# value_vars = what we want to melt\n",
    "pd.melt(frame=df,id_vars = 'COLUMN', value_vars= ['list_of_melting_columns'])\n",
    "\n",
    "## Pivoting (reverse melt)\n",
    "# Index is name\n",
    "# I want to make that columns are variable\n",
    "# Finally values in columns are value\n",
    "df.pivot(index = 'COLUM', columns = 'variable_col',values='value_col')\n",
    "\n",
    "## CONCATENATING DATA\n",
    "conc_data_row = pd.concat([data1,data2],axis =0,ignore_index =True) # axis = 0 : adds dataframes in row\n",
    "\n",
    "# CONVERTING DATA TYPES\n",
    "df['COLUMN'] = df['COLUMN'].astype('category') # object(string),bool, int, float and category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data\n",
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "input_data = np.array([[5.1, -2.9, 3.3],\n",
    "                       [-1.2, 7.8, -6.1],\n",
    "                       [3.9, 0.4, 2.1],\n",
    "                       [7.3, -9.9, -4.5]])\n",
    "\n",
    "# Binarize data \n",
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)\n",
    "\n",
    "# Print mean and standard deviation\n",
    "print(\"\\nBEFORE:\")\n",
    "print(\"Mean =\", input_data.mean(axis=0))\n",
    "print(\"Std deviation =\", input_data.std(axis=0))\n",
    "\n",
    "# Remove mean\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\nAFTER:\")\n",
    "print(\"Mean =\", data_scaled.mean(axis=0))\n",
    "print(\"Std deviation =\", data_scaled.std(axis=0))\n",
    "\n",
    "# Min max scaling\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print(\"\\nMin max scaled data:\\n\", data_scaled_minmax)\n",
    "\n",
    "# Normalize data\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm='l1')\n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm='l2')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)\n",
    "print(\"\\nL2 normalized data:\\n\", data_normalized_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('file_path')\n",
    "# It automatically converts all the categories to dummy featuers\n",
    "df = pd.get_dummies(data)\n",
    "# For binary categories we need to drop one of the dummy columns since they are redundant\n",
    "df.drop(\"dummy_1\",axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Label encoding refers to the process of transforming the word labels into numerical form. This enables the algorithms to operate on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach: using preprocessing module\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb_encoder = LabelEncoder()\n",
    "df[\"COLUMN_CAT\"] = lb_encoder.fit_transform(df[\"COLUMN\"])\n",
    "\n",
    "# Creating dummy varaibles for categorical columns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb_style = LabelBinarizer()\n",
    "lb_results = lb_style.fit_transform(df[\"COLUMN\"])\n",
    "pd.DataFrame(lb_results, columns=lb_style.classes_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second approach: using pandas to convert the column type to category\n",
    "df['COLUMN'] = df['COLUMN'].astype('category')\n",
    "df['COLUMN_CATEGORY'] = df['COLUMN_CATEGORY'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third approach: mapping manually\n",
    "df['COLUMN'] = df['COLUMN'].map( {'VALUE_1': 0, 'VALUE_2': 1, 'VALUE_3': 2} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string data to numerical data\n",
    "# imagine data is a list of lists or a np.array\n",
    "label_encoder = []\n",
    "\n",
    "# First construct the transformed data skeleton\n",
    "X_encoded = np.empty(data.shape)\n",
    "\n",
    "for i, item in enumerate(data[0]): \n",
    "      if item.isdigit(): \n",
    "            X_encoded[:, i] = data[:, i] \n",
    "      else: \n",
    "            label_encoder.append(preprocessing.LabelEncoder()) \n",
    "            X_encoded[:, i] = label_encoder[-1].fit_transform(data[:, i]) \n",
    "            \n",
    "# imagine last column is the outcome (y)\n",
    "X = X_encoded[:, :-1].astype(int) \n",
    "y = X_encoded[:, -1].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize input data \n",
    "plt.figure()\n",
    "plt.scatter(X, \n",
    "            y, \n",
    "            s=75, \n",
    "            facecolors='white',\n",
    "            edgecolors='black', \n",
    "            linewidth=1, \n",
    "            marker='s')\n",
    "plt.title('Input data') \n",
    "\n",
    "## Line plot\n",
    "df.COL1.plot(kind = 'line', \n",
    "             color = 'g',\n",
    "             label = 'LABEL',\n",
    "             linewidth=1,\n",
    "             alpha = 0.5,\n",
    "             grid = True,\n",
    "             linestyle = ':')\n",
    "# add more line into same plot\n",
    "df.COL2.plot(color = 'r',\n",
    "             label = 'Defense',\n",
    "             linewidth=1, \n",
    "             alpha = 0.5,\n",
    "             grid = True,\n",
    "             linestyle = '-.')\n",
    "plt.legend(loc='upper right')     # legend = puts label into plot\n",
    "plt.xlabel('x axis')              # label = name of label\n",
    "plt.ylabel('y axis')\n",
    "plt.title('Line Plot')            # title = title of plot\n",
    "\n",
    "## Histogram\n",
    "# bins = number of bar in figure\n",
    "df.COL.plot(kind = 'hist',bins = 10)\n",
    "\n",
    "## Boxplot\n",
    "df.boxplot(column='y_column',by = 'x_column')\n",
    "\n",
    "## Ploting tricks\n",
    "\n",
    "## Using a scatter plot to draw graph with multiple marker\n",
    "plt.figure() \n",
    "plt.title('Multiple markers') \n",
    "marker_shapes = 'v^os' \n",
    "mapper = [marker_shapes[i] for i in range(0,4)] \n",
    "for i in range(df.shape[0]): \n",
    "    plt.scatter(df[i, 0], df[i, 1], marker=mapper[i],  \n",
    "            s=75, edgecolors='black', facecolors='none') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression classifier\n",
    "classifier = linear_model.LogisticRegression(solver='liblinear', C=1)\n",
    "#classifier = linear_model.LogisticRegression(solver='liblinear', C=100)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X, y)\n",
    "accuracy_NB = nb_clf.score(features_test, outcomes_test)\n",
    "print(accuracy_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model which can be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# Model persistence \n",
    "output_model_file = 'model.pkl' \n",
    " \n",
    "# Save the model \n",
    "with open(output_model_file, 'wb') as f: \n",
    "    pickle.dump(model_obj, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm \n",
    "\n",
    "# Create linear regressor object \n",
    "regressor = LinearRegression() \n",
    " \n",
    "# Train the model using the training sets \n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "# Predict the output \n",
    "y_test_pred = regressor.predict(X_test) \n",
    "\n",
    "# Compute performance metrics \n",
    "print(\"Linear regressor performance:\") \n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_test_pred), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_test_pred), 2))  \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_test_pred), 2))  \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_test_pred), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, y_test_pred), 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avioding overfitting with splitting data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1)\n",
    "\n",
    "\n",
    "## Cross-validtion KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(df):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation recommendations\n",
    " - K can be any number, but K=10 is generally recommended\n",
    " - For classification problems, **stratified sampling** is recommended for creating the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Model could be  any model, Regression, kNN, DecisionTrees ...\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another method to have stratified cross-validated sampling whould be as follows:\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(df_features, df_outcome):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regression\n",
    "As we learn linear regression choose parameters (coefficients) while minimizing lost function. If linear regression thinks that one of the feature is important, it gives high coefficient to this feature. However, this can cause overfitting that is like memorizing in KNN. In order to avoid overfitting, we use regularization that penalize large coefficients.\n",
    "\n",
    "$$ OLS = Ordinary Least Square = SumAllResidulas $$\n",
    "$$ L1: LessoRegressionLostFunction = OLS +\\ alpha \\times \\sum\\lvert parameter\\rvert $$\n",
    "\n",
    "$$ L2: RidgeRegressionLostFuction = OLS +\\ alpha \\times \\sum(parameter)^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Approach: Ridge regression or L2\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,\n",
    "                                                 y,\n",
    "                                                 random_state=2, \n",
    "                                                 test_size=0.3)\n",
    "# alpha is a hyperparameter(0,1): small alpha => overfitting, large => underfitting\n",
    "ridge = Ridge(alpha = 0.1, normalize = True)\n",
    "ridge.fit(x_train,y_train)\n",
    "ridge_predict = ridge.predict(x_test)\n",
    "print('Ridge score: ',ridge.score(x_test,y_test))\n",
    "\n",
    "## Second Approach: Lesso regression or L1\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,\n",
    "                                                 y,\n",
    "                                                 random_state=3, \n",
    "                                                 test_size=0.3)\n",
    "lasso = Lasso(alpha = 0.1, normalize = True)\n",
    "lasso.fit(x_train,y_train)\n",
    "ridge_predict = lasso.predict(x_test)\n",
    "print('Lasso score: ',lasso.score(x_test,y_test))\n",
    "print('Lasso coefficients: ',lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and ROC Curve\n",
    "- Logistic regression output is probabilities.\n",
    "- By default logistic regression threshold is 0.5\n",
    "- In ROC curve, x axis is false positive rate and y axis is true positive rate.\n",
    "- If the curve in plot is closer to left-top corner, test is more accurate.\n",
    "- Roc curve score is auc that is computation area under the curve from prediction scores: We want auc to closer 1.\n",
    "- **fpr** = False Positive Rate\n",
    "- **tpr** = True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve with logistic regression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state=42)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred_prob = logreg.predict_proba(x_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "# Decision Trees classifier \n",
    "# random_state: The seed used by the random number generator required for algorithm\n",
    "# max_depth: the maximum depth of the tree that we want to construct\n",
    "params = {'random_state': 0, 'max_depth': 4} \n",
    "classifier = DecisionTreeClassifier(**params) \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "y_test_pred = classifier.predict(X_test) \n",
    "\n",
    "# Evaluate classifier performance: Imagine there are two classes in our data\n",
    "class_names = ['Class-0', 'Class-1'] \n",
    "print(\"\\n\" + \"#\"*40)\n",
    "print(\"\\nClassifier performance on training dataset\\n\") \n",
    "print(classification_report(y_train, \n",
    "                            classifier.predict(X_train), \n",
    "                            target_names=class_names)) \n",
    "print(\"#\"*40 + \"\\n\") \n",
    " \n",
    "print(\"#\"*40) \n",
    "print(\"\\nClassifier performance on test dataset\\n\") \n",
    "print(classification_report(y_test, \n",
    "                            y_test_pred, \n",
    "                            target_names=class_names)) \n",
    "print(\"#\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest / Extremely RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "# The n_estimators parameter refers to the number of trees that will be constructed.\n",
    "params = {'n_estimators': 100, \n",
    "          'max_depth': 4, \n",
    "          'random_state': 0}\n",
    "# Random Forest model\n",
    "classifier = RandomForestClassifier(**params)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Extremely Random Forest model\n",
    "classifier = ExtraTreesClassifier(**params)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# For a classifier to perform well, it needs to see equal number of points for each class. \n",
    "# But when we collect data in the real world, it's not always possible to ensure that \n",
    "# each class has the exact same number of data points. \n",
    "# If one class has 10 times the number of data points of the other class, \n",
    "# then the classifier tends to get biased towards the first class.\n",
    "params = {'n_estimators': 100, \n",
    "          'max_depth': 4, \n",
    "          'random_state': 0, \n",
    "          'class_weight': 'balanced'}\n",
    "classifier = RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## First example: applied on ExtraTreesClassifier\n",
    "# Define the parameter grid  \n",
    "parameter_grid = [{'n_estimators': [100], 'max_depth': [2, 4, 7, 12, 16]},\n",
    "                  {'max_depth': [4], 'n_estimators': [25, 50, 100, 250]}]\n",
    "\n",
    "metrics = ['precision_weighted', 'recall_weighted'] \n",
    "\n",
    "for metric in metrics: \n",
    "      print(\"\\n##### Searching optimal parameters for\", metric) \n",
    " \n",
    "      classifier = GridSearchCV(ExtraTreesClassifier(random_state=0),\n",
    "                                parameter_grid, \n",
    "                                cv=5, \n",
    "                                scoring=metric)\n",
    "    \n",
    "      classifier.fit(X_train, y_train)\n",
    "\n",
    "      print(\"\\nGrid scores for the parameter grid:\") \n",
    "      for params, avg_score, _ in classifier.grid_scores_: \n",
    "            print(params, '-->', round(avg_score, 3)) \n",
    " \n",
    "      print(\"\\nBest parameters:\", classifier.best_params_) \n",
    "\n",
    "      y_pred = classifier.predict(X_test) \n",
    "      print(\"\\nPerformance report:\\n\") \n",
    "      print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "## Second example: applied on kNN\n",
    "grid = {'n_neighbors': np.arange(1,50)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, \n",
    "                      grid, \n",
    "                      cv=3) # GridSearchCV\n",
    "knn_cv.fit(x,y)# Fit\n",
    "\n",
    "# Print hyperparameter\n",
    "print(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \n",
    "print(\"Best score: {}\".format(knn_cv.best_score_))\n",
    "\n",
    "## Third example: applied on Logisitic Regression\n",
    "# 1. hyperparameter is C:logistic regression regularization parameter\n",
    "# 2. penalty l1 or l2\n",
    "# Hyperparameter grid\n",
    "param_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 12)\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(logreg,\n",
    "                         param_grid,\n",
    "                         cv=3)\n",
    "\n",
    "logreg_cv.fit(x_train,y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define sample labels \n",
    "true_labels = [2, 0, 0, 2, 4, 4, 1, 0, 3, 3, 3] \n",
    "pred_labels = [2, 1, 0, 2, 4, 3, 1, 0, 1, 3, 3] \n",
    "# Create confusion matrix \n",
    "confusion_mat = confusion_matrix(true_labels, pred_labels) \n",
    "print(confusion_mat)\n",
    "\n",
    "## Visualize confusion matrix\n",
    "# By using matplot\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.gray) \n",
    "plt.title('Confusion matrix') \n",
    "plt.colorbar() \n",
    "ticks = np.arange(5) \n",
    "plt.xticks(ticks, ticks) \n",
    "plt.yticks(ticks, ticks) \n",
    "plt.ylabel('True labels') \n",
    "plt.xlabel('Predicted labels') \n",
    "plt.show()\n",
    "\n",
    "# by using seaborn\n",
    "import seaborn as sns\n",
    "sns.heatmap(confusion_mat, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision / Recall / f1\n",
    "Lets discuss accuracy. Is it enough for measurement of model selection. For example, there is a data that includes 95% normal and 5% abnormal samples and our model uses accuracy for measurement metric. Then our model predict 100% normal for all samples and accuracy is 95% but it classify all abnormal samples wrong. Therefore we need to use **confusion matrix** as a model measurement matris in **imbalance data**.\n",
    "\n",
    "**Precision:** accuracy of the model:\n",
    "$$ Precision = \\frac {TP}{(TP+FP)} $$\n",
    "\n",
    "**Recall:** the number of items that were retrieved as a percentage of the overall number of items that were supposed to be retrieved.\n",
    "$$ Recall = \\frac {TP}{(TP+FN)} $$\n",
    "\n",
    "Good classifier => High:Precision High:Recall\n",
    "\n",
    "$$ f1 = \\frac {2 \\times Precision \\times Recall}{(Precision + Recall)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Classification report \n",
    "targets = ['Class-0', 'Class-1', 'Class-2', 'Class-3', 'Class-4'] \n",
    "print('\\n', classification_report(true_labels, pred_labels, target_names=targets)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing relative feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "\n",
    "# AdaBoost Regressor model \n",
    "regressor = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),  \n",
    "            n_estimators=400, random_state=7) \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance of AdaBoost regressor \n",
    "y_pred = regressor.predict(X_test) \n",
    "print(\"\\nADABOOST REGRESSOR\") \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_test, y_pred), 2)) \n",
    "print(\"Explained variance score =\", round(explained_variance_score(y_test, y_pred) , 2)) \n",
    "\n",
    "# Extract feature importances \n",
    "feature_importances = regressor.feature_importances_ \n",
    "feature_names = housing_data.feature_names \n",
    "\n",
    "# Sort the values and flip them \n",
    "index_sorted = np.flipud(np.argsort(feature_importances)) \n",
    "\n",
    "# Arrange the X ticks \n",
    "pos = np.arange(index_sorted.shape[0]) + 0.5 \n",
    "\n",
    "# Plot the bar graph \n",
    "plt.figure() \n",
    "plt.bar(pos, feature_importances[index_sorted], align='center') \n",
    "plt.xticks(pos, feature_names[index_sorted]) \n",
    "plt.ylabel('Relative Importance') \n",
    "plt.title('Feature importance using AdaBoost regressor') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create KMeans object\n",
    "# init: use k-means++ to select these centers in a smarter way.\n",
    "# n_init: refers to the number of times the algorithm should run before deciding upon the best outcome.\n",
    "kmeans = KMeans(init='k-means++', n_clusters='SOME_NUMBER', n_init=10) \n",
    "kmeans.fit(df)\n",
    "labels = kmeans.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Hyperparameter for Kmeans\n",
    "**inertia**: How speard out the clusters are distance from each other.\n",
    "**What is the best number of clusters**? There are low inertia and not too many cluster trade off so we can choose elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_list = np.empty(10)\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(df)\n",
    "    inertia_list[i] = kmeans.inertia_\n",
    "plt.plot(range(0,8),inertia_list,'-o')\n",
    "plt.xlabel('Number of cluster')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the number of clusters with Mean Shift algorithm\n",
    "In the Mean Shift algorithm, we consider the whole feature space as a probability density function. We start with the training dataset and assume that they have been sampled from a probability density function. In this framework, the clusters correspond to the local maxima of the underlying distribution. If there are K clusters, then there are K peaks in the underlying data distribution and Mean Shift will identify those peaks.\n",
    "\n",
    "The goal of Mean Shift is to identify the location of centroids.\n",
    "\n",
    "Estimate the bandwidth of the input data. Bandwidth is a parameter of the underlying kernel density estimation process used in Mean Shift algorithm. The bandwidth affects the overall convergence rate of the algorithm and the number of clusters that we will end up with in the end. Hence this is a crucial parameter. If the bandwidth is small, it might results in too many clusters, where as if the value is large, then it will merge distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth \n",
    "\n",
    "# Estimate the bandwidth of X \n",
    "bandwidth_X = estimate_bandwidth(X, quantile=0.1, n_samples=len(X)) \n",
    "\n",
    "# Cluster data with MeanShift \n",
    "meanshift_model = MeanShift(bandwidth=bandwidth_X, bin_seeding=True) \n",
    "meanshift_model.fit(X) \n",
    "\n",
    "# Extract the centers of clusters \n",
    "cluster_centers = meanshift_model.cluster_centers_ \n",
    "print('\\nCenters of clusters:\\n', cluster_centers) \n",
    "\n",
    "# Estimate the number of clusters \n",
    "labels = meanshift_model.labels_ \n",
    "num_clusters = len(np.unique(labels)) \n",
    "print(\"\\nNumber of clusters in input data =\", num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the quality of clustering with silhouette scores\n",
    "$$ silhouette score = (p - q) / max(p, q) $$\n",
    "\n",
    "Here, p is the mean distance to the points in the nearest cluster that the data point is not a part of, and q is the mean intra-cluster distance to all the points in its own cluster.\n",
    "\n",
    "The value of the silhouette score range lies between -1 to 1. A score closer to 1 indicates that the data point is very similar to other data points in the cluster, whereas a score closer to -1 indicates that the data point is not similar to the data points in its cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "# Initialize variables \n",
    "scores = [] \n",
    "values = np.arange(2, 10) \n",
    "\n",
    "# Iterate through the defined range \n",
    "for num_clusters in values: \n",
    "    # Train the KMeans clustering model \n",
    "    kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10) \n",
    "    kmeans.fit(X) \n",
    "\n",
    "    score = metrics.silhouette_score(X, kmeans.labels_,  \n",
    "            metric='euclidean', sample_size=len(X)) \n",
    "\n",
    "    print(\"\\nNumber of clusters =\", num_clusters) \n",
    "    print(\"Silhouette score =\", score)                  \n",
    "    scores.append(score) \n",
    "\n",
    "# Extract best score and optimal number of clusters \n",
    "num_clusters = np.argmax(scores) + values[0] \n",
    "print('\\nOptimal number of clusters =', num_clusters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import samples_generator \n",
    "from sklearn.feature_selection import SelectKBest, f_regression \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Generate data  \n",
    "X, y = samples_generator.make_classification(n_samples=150,  \n",
    "        n_features=25, n_classes=3, n_informative=6,  \n",
    "        n_redundant=0, random_state=7) \n",
    "\n",
    "# Select top K features  \n",
    "k_best_selector = SelectKBest(f_regression, k=9) \n",
    "\n",
    "# Initialize Extremely Random Forests classifier  \n",
    "classifier = ExtraTreesClassifier(n_estimators=60, max_depth=4) \n",
    "\n",
    "# Construct the pipeline \n",
    "processor_pipeline = Pipeline([('selector', k_best_selector), ('erf', classifier)])\n",
    "\n",
    "# Set the parameters \n",
    "processor_pipeline.set_params(selector__k=7, erf__n_estimators=30) \n",
    "\n",
    "# Training the pipeline  \n",
    "processor_pipeline.fit(X, y)\n",
    "\n",
    "# Predict outputs for the input data \n",
    "output = processor_pipeline.predict(X) \n",
    "print(\"\\nPredicted output:\\n\", output) \n",
    "\n",
    "# Print scores  \n",
    "print(\"\\nScore:\", processor_pipeline.score(X, y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python pass function parameters as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dict by having all desired arguments\n",
    "params = {'n_estimators': 100, 'max_depth': 4, 'random_state': 0}\n",
    "# Pass the dict varaible to the function caller by preceding with ** \n",
    "classifier = RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python argument parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Argument parser  \n",
    "def build_arg_parser(): \n",
    "      parser = argparse.ArgumentParser(description='Classify data using Ensemble Learning techniques') \n",
    "      parser.add_argument('--classifier-type', dest='classifier_type',  \n",
    "                  required=True, choices=['rf', 'erf'], help=\"Type of classifier to use; can be either 'rf' or 'erf'\") \n",
    "      return parser \n",
    "    \n",
    "if __name__=='__main__': \n",
    "      # Parse the input arguments \n",
    "      args = build_arg_parser().parse_args() \n",
    "      classifier_type = args.classifier_type\n",
    "\n",
    "      if classifier_type == 'rf': \n",
    "            classifier = RandomForestClassifier(**params) \n",
    "      else: \n",
    "            classifier = ExtraTreesClassifier(**params) \n",
    "\n",
    "## Then call the script like this:\n",
    "$ python3 SCRIPT_NAME.py --classifier-type rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second approach is by using sys package\n",
    "import sys\n",
    "\n",
    "# within script check for arguments passed to the script and do the related actions\n",
    "if len(sys.argv) > 1: \n",
    "      if sys.argv[1] == 'SOME_VALUE': \n",
    "        ## DO SOMETHING\n",
    "      else: \n",
    "        ## DO SOMETHING ELSE\n",
    "\n",
    "## Then call the script like this:\n",
    "$ python3 SCRIPT_NAME.py SOME_VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        items = line[:-1].split(',')\n",
    "        data.append(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas setting for avoiding truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 4000\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print output spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column1                      column2_with_space                       column3\n"
     ]
    }
   ],
   "source": [
    "# print the contents of each training and testing set\n",
    "print('{} {:^61} {}'.format('column1', \n",
    "                            'column2_with_space', \n",
    "                            'column3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Python: 0.819417 sec\n",
      "Naive NumPy: 0.795714 sec\n",
      "Good NumPy: 0.012523 sec\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "normal_py_sec = timeit.timeit('sum(x*x for x in range(1000))',\n",
    "                              number=10000)\n",
    "naive_np_sec = timeit.timeit(\n",
    "                'sum(na*na)',\n",
    "                setup=\"import numpy as np; na=np.arange(1000)\",\n",
    "                number=10000)\n",
    "good_np_sec = timeit.timeit(\n",
    "                'na.dot(na)',\n",
    "                setup=\"import numpy as np; na=np.arange(1000)\",\n",
    "                number=10000)\n",
    "print(\"Normal Python: %f sec\" % normal_py_sec)\n",
    "print(\"Naive NumPy: %f sec\" % naive_np_sec)\n",
    "print(\"Good NumPy: %f sec\" % good_np_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second approach: using jupyter notebook magic functions\n",
    "# means loop 100 times the following statements in the current cell\n",
    "%%timeit -n 100\n",
    "# TO SOMETHING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
