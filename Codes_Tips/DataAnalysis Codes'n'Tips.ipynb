{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data by having a glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df)\n",
    "dim(df)\n",
    "describe(df)\n",
    "\n",
    "# Unique values per column\n",
    "lapply(df, function(x) length(unique(x)))\n",
    "\n",
    "# Fast way to check NAs in each column of dataframe\n",
    "colSums(is.na(df))\n",
    "# or (as alternative)\n",
    "sapply(df, function(x)sum(is.na(x)))\n",
    "       \n",
    "#Check the percentage of Missing values       \n",
    "missing_values <- df %>% summarize_all(funs(sum(is.na(.))/n()))\n",
    "missing_values <- gather(missing_values, key=\"feature\", value=\"missing_pct\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count the occurrence of each value\n",
    "table(df$colname)\n",
    "\n",
    "# To get the percentage of occurence\n",
    "prop.table(table(df$colname))\n",
    "       \n",
    "# In order to get most frequent value in a column of df\n",
    "tail(names(sort(table(df$colname))), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion table for two categorical variable in dataset\n",
    "with(df, CrossTable(colname1, colname2))\n",
    "\n",
    "# In order to show relation between two contiuous variable we can do the scatterplot\n",
    "ggplot(data = df, aes(x = colname1 , y = colname2)) + geom_point()\n",
    "\n",
    "# For a combination of continuous and categorical variables we can use boxplot\n",
    "ggplot(data = df, aes(x = colname1 , y = colname2)) + geom_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach for spoting outliners are the using geom_jitter\n",
    "ggplot(df, aes(colname1, colname2)) + \n",
    "    geom_jitter(alpha = 0.2 ,size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming field names by using dplyr\n",
    "df <- df %>% rename(\n",
    "  newname1 = oldname1,\n",
    "  newname2 = oldname2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First approach: simply replace NAs with mean of the column\n",
    "df <- df %>%\n",
    "    mutate( colname = ifelse(is.na(colname), mean(df$colname, na.rm=TRUE), colname))\n",
    "\n",
    "# Use the most common value to replace NAs in the desired feature. Imagine S was the most frequent \n",
    "# value for that field within the dataframe \n",
    "df$colname <- replace(df$colname, which(is.na(df$colname)), 'S')\n",
    "\n",
    "# One sublte way to filling null values in data set would be using\n",
    "# prediction based on other explanatory variables. For example, \n",
    "# here we're looking for a countinous variable based on others.\n",
    "exp_var_filler_model <- rpart(exp_var_with_many_NA ~ a_list_of_exp_vars\n",
    "                              data=df[!is.na(df$exp_var),], \n",
    "                              method=\"anova\" #since we consider that exp_var as continous, otherwise we could use `class`)\n",
    "df$exp_var[is.na(df$exp_var)] <- predict(exp_var_filler_model, \n",
    "                                         [is.na(df$exp_var),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a random sample dataframe from the original data by having only 3 row[for sake of example]\n",
    "df_sample <- df[sample(nrow(df), 3), ]\n",
    "\n",
    "# Using dplyr to have the same result as above\n",
    "df_sample <- sample_n(df, size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Feature engineering has been described as easily the most important factor in determining the success or failure of your predictive model. Feature engineering really boils down to the human element in machine learning. How much you understand the data, with your human intuition and creativity, can make the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- df %>%\n",
    "    mutate( new_feature = case_when(raw_feature < 13 ~ \"Lower.Range\", \n",
    "                                    raw_feature >= 13 & raw_feature < 18 ~ \"Mid.Range.1\",\n",
    "                                    raw_feature >= 18 & raw_feature < 60 ~ \"Mid.Range.2\",\n",
    "                                    raw_feature >= 60 ~ \"Upper.Range\"))\n",
    "\n",
    "# In order to combine same categorical data which has very little proportion from total we can use recode method\n",
    "libray(car)\n",
    "df$cat_var <- recode(df$cat_var, \"c('group1', 'group2',...) = 'more_general_group'\")\n",
    "# Or simply\n",
    "df3$cat_var[df$cat_var %in% c('group1','group2')] = 'more_general_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of survivors for the different subsets\n",
    "> aggregate(Survived ~ Child + Sex, data=train, FUN=sum)\n",
    "  Child    Sex Survived\n",
    "1     0 female      195\n",
    "2     1 female       38\n",
    "3     0   male       86\n",
    "4     1   male       23\n",
    "\n",
    "# To know the total number of people in each subset\n",
    "> aggregate(Survived ~ Child + Sex, data=train, FUN=length)\n",
    "  Child    Sex Survived\n",
    "1     0 female      259\n",
    "2     1 female       55\n",
    "3     0   male      519\n",
    "4     1   male       58\n",
    "\n",
    "# To know the proportions\n",
    "> aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})\n",
    "  Child    Sex  Survived\n",
    "1     0 female 0.7528958\n",
    "2     1 female 0.6909091\n",
    "3     0   male 0.1657033\n",
    "4     1   male 0.3965517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Plot\n",
    "The corrplot package is a graphical display of a correlation matrix, confidence interval. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df %>%\n",
    "  select_if(is.numeric) %>%\n",
    "  cor(use=\"complete.obs\") %>%\n",
    "  corrplot.mixed(tl.cex=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "# Using rpart for list of explanatory variables for a discreate outcome\n",
    "# class method (for ones and zeros output)\n",
    "rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "               data=df_train,\n",
    "               method=\"class\")\n",
    "\n",
    "# If you wanted to predict a continuous variable as outcome\n",
    "rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "               data=df_train,\n",
    "               method=\"anova\")\n",
    "\n",
    "# Making prediction on test dataset\n",
    "Prediction <- predict(fit, test, type = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "Overfitting is technically defined as a model that performs better on a training set than another simpler model, but does worse on unseen data.\n",
    "\n",
    "Use caution with decision trees, and any other algorithm actually, or you can find yourself making rules from the noise youâ€™ve mistaken for signal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options to optimise overfitting in decision trees\n",
    "# cp determines the complexity control and minsplit governs \n",
    "# the minimum required members for each split \n",
    "rpart(Outcome ~ exp_var1 + exp_var2 + ...,\n",
    "               data=df_train,\n",
    "               method=\"class\", \n",
    "               control=rpart.control(minsplit=2, cp=0))\n",
    "# To trim trees manually in R we can use:\n",
    "new.fit <- prp(fit,snip=TRUE)$obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Based on problem:\n",
    "    - If it's a classification problem, then we can use:\n",
    "        * Logistic Regression\n",
    "        * Naive Bayes\n",
    "        * Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "Take a large collection of individually imperfect models, and their one-off mistakes are probably not going to be made by the rest of them. If we average the results of all these models, we can sometimes find a superior model from their combination than any of the individual parts. Thatâ€™s how ensemble models work, they grow a lot of different models, and let their outcomes be averaged or voted across the group.\n",
    "\n",
    "### Random Forest\n",
    "Random Forest models grow trees much deeper than the decision stumps above, in fact the default behaviour is to grow each tree out as far as possible. But since the formulas for building a single decision tree are the same every time, some source of randomness is required to make these trees different from one another. Random Forests do this in two ways.\n",
    " 1. The first trick is to use bagging, for bootstrap aggregating. Bagging takes a randomized sample of the rows in your training set, with replacement.\n",
    " 2. The second source of randomness gets past this limitation though. Instead of looking at the entire pool of available variables, Random Forests take only a subset of them, typically the square root of the number available. In our case we have 10 variables, so using a subset of three variables would be reasonable. The selection of available variables is changed for each and every node in the decision trees. \n",
    " \n",
    "Râ€™s Random Forest algorithm has a few restrictions that we did not have with our decision trees. The big one has been the elephant in the room until now, we have to clean up the missing values in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an random forest model after data prepration process\n",
    "fit <- randomForest(as.factor(outcome) ~ exp_var1 + exp_var2 + ...,\n",
    "                    data=df_train, \n",
    "                    importance=TRUE, \n",
    "                    ntree=2000)\n",
    "\n",
    "# So letâ€™s look at what variables were important:\n",
    " varImpPlot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereâ€™s two types of importance measures shown above. The accuracy one tests to see how worse the model performs without each variable, so a high decrease in accuracy would be expected for very predictive variables. The Gini one digs into the mathematics behind decision trees, but essentially measures how pure the nodes are at the end of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction <- predict(fit, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But letâ€™s not give up yet. Thereâ€™s more than one ensemble model. Letâ€™s try a forest of conditional inference trees. They make their decisions in slightly different ways, using a statistical test rather than a purity measure, but the basic construction of each tree is fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('party')\n",
    "library(party)\n",
    "\n",
    "fit <- cforest(as.factor(outcome) ~ exp_var1 + exp_var2 + ...,\n",
    "                 data = df_train, \n",
    "                 controls=cforest_unbiased(ntree=2000, mtry=3))\n",
    "\n",
    "Prediction <- predict(fit, test, OOB=TRUE, type = \"response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
